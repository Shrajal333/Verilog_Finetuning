{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NYBPT5ikFyO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install Libraries\n",
        "!pip install ninja libaio triton transformers datasets accelerate peft bitsandbytes trl sentencepiece deepspeed loralib wandb huggingface_hub git-lfs auto-gptq optimum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing flash-attention\n",
        "!MAX_JOBS=8 pip install flash-attn --no-build-isolation #2.2 12.1.1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1mvYey1eLKYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make directory required for caching kernels\n",
        "!mkdir -p /root/.triton/autotune"
      ],
      "metadata": {
        "id": "R0WMFYw5it4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxn8ZRxaYWW4"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Set environment variables for stability\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Model Name (Smaller 3B CodeQwen)\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-Coder-1.5B\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"verilog_autocomplete.jsonl\")[\"train\"].train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHYyzi8IYWSo"
      },
      "outputs": [],
      "source": [
        "# 4-bit Quantization Configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load Model with 4-bit Quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.use_sliding_window_attention = False\n",
        "\n",
        "# Load tokenizer and set padding to left\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.padding_side = \"left\"  # Required for FlashAttention\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Ensure a valid pad token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    texts = [str(msg) for msg in examples[\"messages\"]]\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "IlVET8s7tTo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku1oCapIYWQh"
      },
      "outputs": [],
      "source": [
        "# Apply QLoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_LPZQa9YWOU"
      },
      "outputs": [],
      "source": [
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"qwen_finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=16,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=2500,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"logs\",\n",
        "    logging_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_bnb_8bit\",\n",
        "    warmup_steps=250,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_safetensors=True,\n",
        "    gradient_checkpointing=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utpnd-JkYWMQ"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"qwen_finetuned\")\n",
        "tokenizer.save_pretrained(\"qwen_finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWOs8kemexfh"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(model_name, tokenizer_name, dataset_path):\n",
        "    \"\"\"Calculates perplexity for a given model on a dataset.\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_path)[\"train\"]\n",
        "\n",
        "    # Tokenize text\n",
        "    # Flatten lists of messages into a single text sequence\n",
        "    messages_text = [\"\\n\".join(msg) if isinstance(msg, list) else msg for msg in dataset[\"messages\"]]\n",
        "    encodings = tokenizer(\"\\n\".join(messages_text), return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    messages_text = [\"\\n\".join([msg[\"content\"] for msg in messages]) for messages in dataset[\"messages\"]]\n",
        "    encodings = tokenizer(\"\\n\".join(messages_text), return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Compute loss (perplexity)\n",
        "    with torch.no_grad():\n",
        "        input_ids = encodings.input_ids.to(device)\n",
        "        attention_mask = encodings.attention_mask.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss.item()\n",
        "\n",
        "    perplexity = math.exp(loss)\n",
        "    print(f\"Model: {model_name} - Perplexity: {perplexity}\")\n",
        "    return perplexity\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = \"/workspace/verilog_autocomplete.jsonl\"\n",
        "\n",
        "# Define model & tokenizer paths\n",
        "original_model = \"Qwen/Qwen2.5-Coder-1.5B\"\n",
        "original_tokenizer = \"Qwen/Qwen2.5-Coder-1.5B\"\n",
        "\n",
        "fine_tuned_model = \"qwen_finetuned\"\n",
        "fine_tuned_tokenizer = \"qwen_finetuned\"\n",
        "\n",
        "# Calculate perplexity\n",
        "original_ppl = calculate_perplexity(original_model, original_tokenizer, dataset_path)\n",
        "fine_tuned_ppl = calculate_perplexity(fine_tuned_model, fine_tuned_tokenizer, dataset_path)\n",
        "\n",
        "if fine_tuned_ppl < original_ppl:\n",
        "    print(\"✅ Fine-tuned model has lower perplexity (better).\")\n",
        "else:\n",
        "    print(\"⚠️ Fine-tuned model has higher perplexity (may need more tuning).\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}